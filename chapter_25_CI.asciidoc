[[chapter_25_CI]]
== CI: Continuous Integration


.Warning, Chapter Update in Progress
*******************************************************************************
ðŸš§ Warning, this chapter is very much a rough sketch at the moment. ðŸš§

Work in progress: switch away from Jenkins,
to something more modern.
I'm still going to avoid the obvious choice of GitHub Actions,
I'm hoping to pick something that's at least open source (eg GitLab),
if not fully self-hosted / free software (like Woodpecker CI).

Also planning to add the CD part, ie automated deployments.
watch this spaceeee

*******************************************************************************


((("Continuous Integration (CI)", id="CI24")))
((("Continuous Integration (CI)", "benefits of")))
As our site grows, it takes longer and longer to run all of our functional tests.
If this continues, the danger is that we're going to stop bothering.

Rather than let that happen, we can automate the running of functional tests
by setting up "Continuous Integration", or CI.
That way, in day-to-day development,
we can just run the FT that we're working on at that time,
and rely on CI to run all the other tests automatically
and let us know if we've broken anything accidentally.

The unit tests should stay fast enough that we can keep running
the full suite locally, every few seconds.

NOTE: Continuous Integration is another practice that was popularised by
    Kent Beck's
    https://martinfowler.com/bliki/ExtremeProgramming.html[Extreme Programming (XP)]
    movement in the 1990s.

As we'll see, one of the great frustrations of CI
is that the feedback loop is so much slower than working locally.
As we go along, we'll look for ways to optimise for that, where we can.

=== Choosing a CI service

((("Continuous Integration (CI)", "choosing a service")))
TODO chat re gha, hosted ci solutions, picked a relatable one.


=== Getting our Code into GitLab

GitLab is primarily a code hosting service like GitHub,
so the first thing thing to do is get our code up there


==== Starting a Project

Use the **New Project** -> **Create Blank Project** option, as in <<gitlab-new-blank-project>>.

.Creating a New Repo on Gitlab
[[gitlab-new-blank-project]]
image::images/gitlab_new_blank_project.png["New Blank Project"]


==== Pushing our Code up using Git Push

First we set up gitlab as a "remote" for our project:

[role="skipme"]
[subs="specialcharacters,quotes"]
----
# substitute your username and project name as necessary
$ *git remote add origin git@gitlab.com:yourusername/superlists.git*
# or, if you already have "origin" defined:
$ *git remote add gitlab git@gitlab.com:yourusername/superlists.git*
$ *git remote -v*
gitlab	git@gitlab.com:hjwp/superlistz.git (fetch)
gitlab	git@gitlab.com:hjwp/superlistz.git (push)
origin	git@github.com:hjwp/book-example.git (fetch)
origin	git@github.com:hjwp/book-example.git (push)
----

Now we can push up our code with `git push`.
The `-u` flag sets up a "remote-tracking" branch,
so you can push+pull without explicitly specifying the remote + branch

[role="skipme"]
[subs="specialcharacters,quotes"]
----
# if using 'origin', it will be the default:
$ *git push -u*
# or, if you have multiple remotes:
$ *git push -u gitlab*
Enumerating objects: 706, done.
Counting objects: 100% (706/706), done.
Delta compression using up to 11 threads
Compressing objects: 100% (279/279), done.
Writing objects: 100% (706/706), 918.72 KiB | 131.25 MiB/s, done.
Total 706 (delta 413), reused 682 (delta 408), pack-reused 0 (from 0)
remote: Resolving deltas: 100% (413/413), done.
To gitlab.com:hjwp/superlistz.git
 * [new branch]        main -> main
branch 'main' set up to track 'gitlab/main'.
----

If you refresh the GitLab UI, you should now see your code, as in <<gitlab_files_ui>>:

.CI Project Files on GitLab
[[gitlab_files_ui]]
image::images/gitlab_files_ui.png["GitLab UI showing project files"]


=== Setting up a First Cut of a CI Pipeline

TODO brief expl. "pipeline" terminology.


Got to **Build** -> **Pipelines**, you'll see a list of example templates.
That's usually a pretty good place to start
when getting to know a new configuration language.
I chose the **Python** one and made a few customisations:


[role="sourcecode"]
..gitlab-ci.yml
====
[source,yaml]
----
# Use the same image as our Dockerfile
image: python:slim

# These two setting lets us cache pip-installed packages,
# it came from the default template
variables:
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"
cache:
  paths:
    - .cache/pip

# "setUp" phase, before the main build
before_script:
  - python --version ; pip --version  # For debugging
  - pip install virtualenv
  - virtualenv .venv
  - source .venv/bin/activate

# This is the main build
test:
  script:
    - pip install -r requirements.txt  # <1>
    - pip install selenium  # <1>
    # unit tests
    - python src/manage.py test lists accounts  # <2>
    # (if those pass) all tests, incl. functional. yes we need that cd.
    - cd src && python manage.py test  # <3>

----
====

YAML once again folks!

<1> Here's where I install our requirements,
    including a manual step to install the non-production / test-only requirements

<2> I've decided to run the unit tests first.
    This gives us an "early failure" if  there's any problem at this stage,
    and saves us from having to run, and more importantly wait for, the Fts to run.

<3> And here is a full test run, including the functional tests.


TIP: It's a good idea in CI pipelines to try and run the quickest tests first,
    so that you can get feedback as quickly as possible.


If you used the web UI to edit your pipeline YAML,
you can go check for results straight away.
If you edited it locally (it's just a file in the repo like any other),
then you'll need to `git push` up to gitlab,
and then go check the **Jobs** section in the **Build** UI.



=== First Build!  (and First Failure)

// TODO: consider deliberately forgetting to pip install selenium

However you click through the UI and you should be able to find your way
to see the output of the build Job.


[role="skipme small-code"]
----
Running with gitlab-runner 17.7.0~pre.103.g896916a8 (896916a8)
  on green-1.saas-linux-small-amd64.runners-manager.gitlab.com/default
  JLgUopmM, system ID: s_deaa2ca09de7
Preparing the "docker+machine" executor 00:20
Using Docker executor with image python:latest ...
Pulling docker image python:latest ...
[...]
$ python src/manage.py test lists accounts
Creating test database for alias 'default'...
Found 55 test(s).
System check identified no issues (0 silenced).
................../builds/hjwp/book-example/.venv/lib/python3.13/site-packages/django/core/handlers/base.py:61: UserWarning: No directory at: /builds/hjwp/book-example/src/static/
  mw_instance = middleware(adapted_handler)
.....................................
 ---------------------------------------------------------------------
Ran 55 tests in 0.129s
OK
Destroying test database for alias 'default'...
$ cd src && python manage.py test
Creating test database for alias 'default'...
Found 63 test(s).
System check identified no issues (0 silenced).
......../builds/hjwp/book-example/.venv/lib/python3.13/site-packages/django/core/handlers/base.py:61: UserWarning: No directory at: /builds/hjwp/book-example/src/static/
  mw_instance = middleware(adapted_handler)
...............................................EEEEEEEE
======================================================================
ERROR: test_layout_and_styling (functional_tests.test_layout_and_styling.LayoutAndStylingTest.test_layout_and_styling)
 ---------------------------------------------------------------------
Traceback (most recent call last):
  File "/builds/hjwp/book-example/src/functional_tests/base.py", line 30, in setUp
    self.browser = webdriver.Firefox()
                   ~~~~~~~~~~~~~~~~~^^

[...]
selenium.common.exceptions.WebDriverException: Message: Process unexpectedly closed with status 255
 ---------------------------------------------------------------------
Ran 63 tests in 8.658s
FAILED (errors=8)

selenium.common.exceptions.WebDriverException: Message: Process unexpectedly closed with status 255
----

You can see we got through the unit tests,
and then in the full test run we have 8 errors out of 63 tests.
The FTs are all failing.

I'm "lucky" because I've done this sort of thing many times before,
so I know what to expect:  it's failing because Firefox isn't installed
in the image we're using.


Let's modify the `before_script` section of our file,

[role="sourcecode"]
..gitlab-ci.yml
====
[source,yaml]
  before_script:
    # install firefox
    - apt update -y && apt install -y firefox-esr  # <1>
    - python --version ; pip --version  # For debugging
    - pip install virtualenv
    - virtualenv .venv
    - source .venv/bin/activate
----
====

<1> We use the Debian Linux `apt` package manager to install Firefox.
    `firefox-esr` is the "extended support release",
    which is a more stable version of Firefox to test against.

If you run it again, and wait a bit, you'll see we get a slightly different failure:


[role="skipme small-code"]
----
$ apt-get update -y && apt-get install -y firefox-esr
Get:1 http://deb.debian.org/debian bookworm InRelease [151 kB]
Get:2 http://deb.debian.org/debian bookworm-updates InRelease [55.4 kB]
Get:3 http://deb.debian.org/debian-security bookworm-security InRelease [48.0 kB]
Get:4 http://deb.debian.org/debian bookworm/main amd64 Packages [8792 kB]
Get:5 http://deb.debian.org/debian bookworm-updates/main amd64 Packages [13.5 kB]
Get:6 http://deb.debian.org/debian-security bookworm-security/main amd64 Packages [245 kB]
Fetched 9305 kB in 1s (9127 kB/s)
Reading package lists...
Reading package lists...
Building dependency tree...
Reading state information...
[...]
The following NEW packages will be installed:
  adwaita-icon-theme alsa-topology-conf alsa-ucm-conf at-spi2-common
  at-spi2-core dbus dbus-bin dbus-daemon dbus-session-bus-common
  dbus-system-bus-common dbus-user-session dconf-gsettings-backend
  dconf-service dmsetup firefox-esr fontconfig fontconfig-config
[...]
Get:117 http://deb.debian.org/debian-security bookworm-security/main amd64
firefox-esr amd64 128.7.0esr-1~deb12u1 [69.8 MB]
[...]
Selecting previously unselected package firefox-esr.
Preparing to unpack .../105-firefox-esr_128.7.0esr-1~deb12u1_amd64.deb ...
Adding 'diversion of /usr/bin/firefox to /usr/bin/firefox.real by firefox-esr'
Unpacking firefox-esr (128.7.0esr-1~deb12u1) ...
[...]
Setting up firefox-esr (128.7.0esr-1~deb12u1) ...
update-alternatives: using /usr/bin/firefox-esr to provide
/usr/bin/x-www-browser (x-www-browser) in auto mode
[...]

ERROR: test_multiple_users_can_start_lists_at_different_urls
(functional_tests.test_simple_list_creation.NewVisitorTest.test_multiple_users_can_start_lists_at_different_urls)
 ---------------------------------------------------------------------
Traceback (most recent call last):
  File "/builds/hjwp/book-example/src/functional_tests/base.py", line 30, in setUp
    self.browser = webdriver.Firefox()
                   ~~~~~~~~~~~~~~~~~^^
[...]
selenium.common.exceptions.WebDriverException: Message: Process unexpectedly
closed with status 1
 ---------------------------------------------------------------------
Ran 63 tests in 3.654s
FAILED (errors=8)
----

We can see Firefox installing OK, but we still get an error.
This time it's exit code 1.


* TODO: write up a debugging session here.


The answer is that Firefox is crashing because it can't find a display.
Servers are "headless", meaning they don't have a screen.
Thankfully Firefox has a "headless' mode,
which we can enable by setting an environment variable.

[role="sourcecode"]
..gitlab-ci.yml
====
[source,yaml]
----
variables:
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"
  # make firefox run without a display
  MOZ_HEADLESS: "1"
----
====


=== A Common Bugbear: Flaky tests

The next error I saw was unexpected:

[role="skipme small-code"]
----
+ python manage.py test functional_tests
......F.
======================================================================
FAIL: test_can_start_a_todo_list
(functional_tests.test_simple_list_creation.NewVisitorTest)
 ---------------------------------------------------------------------
Traceback (most recent call last):
  File "...goat-book/functional_tests/test_simple_list_creation.py", line
38, in test_can_start_a_todo_list
    self.wait_for_row_in_list_table('2: Use peacock feathers to make a fly')
  File "...goat-book/functional_tests/base.py", line 51, in
wait_for_row_in_list_table
    raise e
  File "...goat-book/functional_tests/base.py", line 47, in
wait_for_row_in_list_table
    self.assertIn(row_text, [row.text for row in rows])
AssertionError: '2: Use peacock feathers to make a fly' not found in ['1: Buy
peacock feathers']
 ---------------------------------------------------------------------
----


You might not see this error,
but it's common for the switch to CI to flush out some "flaky" tests,
things that will fail intermittently.
In CI a common cause is the "noisy neighbour" problem,
where the CI server might be much slower than your own machine,
thus flushing out some race conditions, or in this case,
just randomly hanging for a few seconds, taking us past the default timeout.


Let's give ourselves some tools to help debug though.


=== Taking Screenshots

((("Continuous Integration (CI)", "screenshots", id="CIscreen24")))
((("screenshots", id="screen24")))
((("debugging", "screenshots for", id="DBscreen24")))
To be able to debug unexpected failures that happen on a remote server,
it would be good to see a picture of the screen at the moment of the failure,
and maybe also a dump of the HTML of the page.

We can do that using some custom logic in our FT class `tearDown`.
We'll need to do a bit of introspection of `unittest` internals,
a private attribute called `._outcome`,
but this will work:

[role="sourcecode"]
.src/functional_tests/base.py (ch23l006)
====
[source,python]
----
import os
import time
from datetime import datetime
from pathlib import Path
[...]
MAX_WAIT = 5

SCREEN_DUMP_LOCATION = Path(__file__).absolute().parent / "screendumps"
[...]

    def tearDown(self):
        if self._test_has_failed():
            if not SCREEN_DUMP_LOCATION.exists():
                SCREEN_DUMP_LOCATION.mkdir(parents=True)
            self.take_screenshot()
            self.dump_html()
        self.browser.quit()
        super().tearDown()

    def _test_has_failed(self):
        # slightly obscure but couldn't find a better way!
        return self._outcome.result.failures or self._outcome.result.errors
----
====


We first create a directory for our screenshots if necessary.
Then we iterate through all the open browser tabs and pages,
and use a Selenium methods, `get_screenshot_as_file()`
and the attribute `browser.page_source`,
for our image and HTML dumps, respectively:

[role="sourcecode"]
.src/functional_tests/base.py (ch23l007)
====
[source,python]
----
    def take_screenshot(self):
        path = SCREEN_DUMP_LOCATION / self._get_filename("png")
        print("screenshotting to", path)
        self.browser.get_screenshot_as_file(str(path))

    def dump_html(self):
        path = SCREEN_DUMP_LOCATION / self._get_filename("html")
        print("dumping page HTML to", path)
        path.write_text(self.browser.page_source)
----
====


And finally here's a way of generating a unique filename identifier,
which includes the name of the test and its class, as well as a timestamp:

[role="sourcecode small-code"]
.src/functional_tests/base.py (ch23l008)
====
[source,python]
----
    def _get_filename(self, extension):
        timestamp = datetime.now().isoformat().replace(":", ".")[:19]
        return (
            f"{self.__class__.__name__}.{self._testMethodName}-{timestamp}.{extension}"
        )
----
====

You can test this first locally by deliberately breaking one of the tests,
with a `self.fail()` for example, and you'll see something like this:

[role="dofirst-ch21l009"]
----
[...]
.Fscreenshotting to ...goat-book/src/functional_tests/screendumps/MyListsTest.t
est_logged_in_users_lists_are_saved_as_my_lists-[...]
dumping page HTML to ...goat-book/src/functional_tests/screendumps/MyListsTest.
test_logged_in_users_lists_are_saved_as_my_lists-[...]
----


=== Saving Build Outputs (or Debug Files) as Artifacts

We also need to tell gitlab to "save" these files
for us to be able to actually look at them
This is called "artifacts":

[role="sourcecode"]
..gitlab-ci.yml
====
[source,yaml]
----
test:
  [...]

  script:
    [...]

  artifacts: # <1>
    when: always  # <2>
    paths: # <1>
      - src/functional_tests/screendumps/
----
====

<1> `artifacts` is the name of the key,
    and the `paths` argument is fairly self-explanatory.
    You can use wildcards here,
    more info in the https://docs.gitlab.com/ci/jobs/job_artifacts/[GitLab docs].

<2> One thing the docs _didn't_ make obvious is that you need `when: always`
    because otherwise it won't save artifacts for failed jobs.
    That was annoyingly hard to figure out!


////
Revert the `self.fail()`, then commit and push:

[role="dofirst-ch21l010"]
[subs="specialcharacters,quotes"]
----
$ *git diff*  # changes in base.py
$ *echo "src/functional_tests/screendumps" >> .gitignore*
$ *git commit -am "add screenshot on failure to FT runner"*
$ *git push*
----
////

In any case that should work.
If you commit the code and then push it back to Gitlab,
we should be able to see a new build job.

In its output, we'll see the screenshots and html dumps being saved:


[role="skipme small-code"]
----
screendumps/LoginTest.test_can_get_email_link_to_log_in-window0-2014-01-22T17.45.12.html
Fscreenshotting to /builds/hjwp/book-example/src/functional_tests/screendumps/NewVisitorTest.test_can_start_a_todo_list-2025-02-17T17.51.01.png
dumping page HTML to /builds/hjwp/book-example/src/functional_tests/screendumps/NewVisitorTest.test_can_start_a_todo_list-2025-02-17T17.51.01.html
Not Found: /favicon.ico
.screenshotting to /builds/hjwp/book-example/src/functional_tests/screendumps/NewVisitorTest.test_multiple_users_can_start_lists_at_different_urls-2025-02-17T17.51.06.png
dumping page HTML to /builds/hjwp/book-example/src/functional_tests/screendumps/NewVisitorTest.test_multiple_users_can_start_lists_at_different_urls-2025-02-17T17.51.06.html
======================================================================
FAIL: test_can_start_a_todo_list (functional_tests.test_simple_list_creation.NewVisitorTest.test_can_start_a_todo_list)
[...]
----


And to the right some new UI options appear to **Browse** the artifacts,
as in <<gitlab_ui_for_browse_artifacts>>.

.Artifacts Appear on the Right of the Build Job
[[gitlab_ui_for_browse_artifacts]]
image::images/gitlab_ui_for_browse_artifacts.png["GitLab UI tab showing the option to browse artifacts"]


And if you navigate through, you'll see something like <<gitlab_ui_show_screenshot>>:

.Our Screenshot in the GitLab UI, Looking Unremarkable
[[gitlab_ui_show_screenshot]]
image::images/gitlab_ui_show_screenshot.png["GitLab UI showing a normal-looking screenshot of the site"]



=== If in Doubt, Try Bumping the Timeout!

((("", startref="CIscreen24")))
((("", startref="screen24")))
((("", startref="DBscreen24")))
((("Continuous Integration (CI)", "timeout bumping")))
((("CI", "timeout bumping")))
Hm.  No obvious clues there.
Well, when in doubt, bump the timeout, as the old adage goes:

[role="sourcecode skipme"]
.src/functional_tests/base.py
====
[source,python]
----
MAX_WAIT = 10
----
====

Then we can rerun the build by pushing, and confirm it now works,


=== A Successful Python Test Run

At this point we should get a working pipeline, <<gitlab_pipeline_success>>:

.A Successful GitLab Pipeline
[[gitlab_pipeline_success]]
image::images/gitlab_pipeline_success.png["GitLab UI showing a successful pipeline run"]




=== Running Our JavaScript Tests in CI

((("Continuous Integration (CI)", "QUnit JavaScript tests", id="CIjs5")))
((("JavaScript testing", "in CI", secondary-sortas="CI", id="JSCI")))
There's a set of tests we almost forgot--the JavaScript tests.
Currently our "test runner" is an actual web browser.q
To get them running in CI, we need a command-line test runner.

* TODO: npm-browser-runner


==== Installing node

It's time to stop pretending we're not in the JavaScript game.
We're doing web development.  That means we do JavaScript.
That means we're going to end up with node.js on our computers.
It's just the way it has to be.

Follow the instructions on the http://nodejs.org/[node.js homepage].
There are installers for Windows and Mac,
and repositories for popular Linux distros.

* TODO: mention nvm


==== Adding A Build Steps for Js

* TODO


((("", startref="CIjs5")))
((("", startref="JSCI")))






=== Tests now pass

And there we are!  A complete CI build featuring all of our tests!



* TODO screenshot

Nice to know that, no matter how lazy I get
about running the full test suite on my own machine, the CI server will catch me.
Another one of the Testing Goat's agents in cyberspace, watching over us...


But, to really finish this off, you should really take a look at <<appendix_CD>>.

I've moved it to an appendix tho, cos it's so gitlab-heavy.



=== Alternatives: Woodpecker and Forgejo

you need your own server for these.
i managed to get forgejo up and running in about 40 minutes.

be careful with security!
these things tend to assume you're on a private network,
or that your code is entirely public.

eg: in forgejo to avoid letting the whole internet sign up and rootle around in your ci (not that anyone would care, really, but, you know)

[role="skipme"]
----
DISABLE_REGISTRATION: true
----


In any case, onto our last chapter!

.Best Practices for CI (including Selenium Tips)
*******************************************************************************

Set up CI as soon as possible for your project::
    As soon as your functional tests take more than a few seconds to run,
    you'll find yourself avoiding running them all.
    Give this job to a CI server,
    to make sure that all your tests are getting run somewhere.
    ((("Selenium", "best CI practices")))
    ((("Continuous Integration (CI)", "tips")))

Optimise for fast feedback::
    CI feedback loops can be frustratingly slow.
    Optimising things to get results quicker is worth the effort.

Set up screenshots and HTML dumps for failures::
    Debugging test failures is easier if you can see what the page looked
    like when the failure occurred.  This is particularly useful for debugging
    CI failures, but it's also very useful for tests that you run locally.
    ((("screenshots")))
    ((("debugging", "screenshots for")))
    ((("HTML", "screenshot dumps")))

Be prepared to bump your timeouts::
    A CI server may not be as speedy as your laptop,
    especially if it's under load, running multiple tests at the same time.
    Be prepared to be even more generous with your timeouts,
    in order to minimise the chance of random failures.
    ((("Flaky tests")))

Take the next step, CD (Continuous Delivery)::
    Once we're running tests automatically,
    we can take the next step which is to automated our deployments
    (when the tests pass). See <<appendix_CD>> for a worked example.
    ((("Continuous Delivery (CD)")))

*******************************************************************************

