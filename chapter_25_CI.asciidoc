[[chapter_25_CI]]
== CICD: Continuous Integration & Continuous Delivery


.Warning, Chapter Update in Progress
*******************************************************************************
ðŸš§ Warning, this chapter is very much a rough sketch at the moment. ðŸš§

Work in progress: switch away from Jenkins,
to something more modern.
I'm still going to avoid the obvious choice of GitHub Actions,
I'm hoping to pick something that's at least open source (eg GitLab),
if not fully self-hosted / free software (like Woodpecker CI).

Also planning to add the CD part, ie automated deployments. 
watch this spaceeee

*******************************************************************************


((("Continuous Integration (CI)", id="CI24")))
((("Continuous Integration (CI)", "benefits of")))
As our site grows, it takes longer and longer to run all of our functional tests.
If this continues, the danger is that we're going to stop bothering.

Rather than let that happen, we can automate the running of functional tests
by setting up "Continuous Integration", or CI.
That way, in day-to-day development,
we can just run the FT that we're working on at that time,
and rely on CI to run all the other tests automatically
and let us know if we've broken anything accidentally.

The unit tests should stay fast enough that we can keep running
the full suite locally, every few seconds.

((("Continuous Integration (CI)", "choosing a service")))
TODO chat re gha, hosted ci solutions, picked a relatable one.


=== Setting up GitLab

* start new ci/cd project, add repo url
* go to build -> pipelines
* use template python project

NOTE: GitLab free tier won't let you "mirror" a repo from elsewhere.
    You either need


change a of things., `/venv` -> `.venv`

ended up with this as first cut:


[role="sourcecode"]
..gitlab-ci.yml
====
[source,yaml]
----
# Official language image. Look for the different tagged releases at:
# https://hub.docker.com/r/library/python/tags/
image: python:latest

# Change pip's cache directory to be inside the project directory since we can
# only cache local items.
variables:
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"

# https://pip.pypa.io/en/stable/topics/caching/
cache:
  paths:
    - .cache/pip

before_script:
  - python --version ; pip --version  # For debugging
  - pip install virtualenv
  - virtualenv .venv
  - source .venv/bin/activate

test:
  script:
    - pip install -r requirements.txt
    - pip install selenium
    # unit tests
    - python src/manage.py test lists accounts
    # (if those pass) all tests, incl. functional. yes we need that cd.
    - cd src && python manage.py test

----
====


=== First Build!

fails as follows:


* TODO: consider deliberately forgetting to pip install selenium

----
[...]
Ran 63 tests in 8.658s
FAILED (errors=8)

selenium.common.exceptions.WebDriverException: Message: Process unexpectedly closed with status 255
----

unit tsets pass but fts are failing

firefox isn't installed.


[role="skipme small-code"]
----
$ apt-get update -y && apt-get install -y firefox-esr
Get:1 http://deb.debian.org/debian bookworm InRelease [151 kB]
Get:2 http://deb.debian.org/debian bookworm-updates InRelease [55.4 kB]
Get:3 http://deb.debian.org/debian-security bookworm-security InRelease [48.0 kB]
Get:4 http://deb.debian.org/debian bookworm/main amd64 Packages [8792 kB]
Get:5 http://deb.debian.org/debian bookworm-updates/main amd64 Packages [13.5 kB]
Get:6 http://deb.debian.org/debian-security bookworm-security/main amd64 Packages [245 kB]
Fetched 9305 kB in 1s (9127 kB/s)
Reading package lists...
Reading package lists...
Building dependency tree...
Reading state information...
[...]
The following NEW packages will be installed:
  adwaita-icon-theme alsa-topology-conf alsa-ucm-conf at-spi2-common
  at-spi2-core dbus dbus-bin dbus-daemon dbus-session-bus-common
  dbus-system-bus-common dbus-user-session dconf-gsettings-backend
  dconf-service dmsetup firefox-esr fontconfig fontconfig-config
[...]
Get:117 http://deb.debian.org/debian-security bookworm-security/main amd64
firefox-esr amd64 128.7.0esr-1~deb12u1 [69.8 MB]
[...]
Selecting previously unselected package firefox-esr.
Preparing to unpack .../105-firefox-esr_128.7.0esr-1~deb12u1_amd64.deb ...
Adding 'diversion of /usr/bin/firefox to /usr/bin/firefox.real by firefox-esr'
Unpacking firefox-esr (128.7.0esr-1~deb12u1) ...
[...]
Setting up firefox-esr (128.7.0esr-1~deb12u1) ...
update-alternatives: using /usr/bin/firefox-esr to provide
/usr/bin/x-www-browser (x-www-browser) in auto mode
[...]

ERROR: test_multiple_users_can_start_lists_at_different_urls
(functional_tests.test_simple_list_creation.NewVisitorTest.test_multiple_users_can_start_lists_at_different_urls)
 ---------------------------------------------------------------------
Traceback (most recent call last):
  File "/builds/hjwp/book-example/src/functional_tests/base.py", line 30, in setUp
    self.browser = webdriver.Firefox()
                   ~~~~~~~~~~~~~~~~~^^
[...]
selenium.common.exceptions.WebDriverException: Message: Process unexpectedly
closed with status 1
 ---------------------------------------------------------------------
Ran 63 tests in 3.654s
FAILED (errors=8)
----

Ooops still not.



[role="sourcecode"]
..gitlab-ci.yml
====
[source,yaml]
----
variables:
  # Change pip's cache directory to be inside the project directory since we can
  # only cache local items.
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.cache/pip"
  # make firefox run without a display
  MOZ_HEADLESS: "1"
----
====


and insert success image.



* TODO: can we show some sort of failure, maybe a flaky test here?


=== Alternatively: forgejo

DISABLE_REGISTRATION: true


=== Taking Screenshots

((("Continuous Integration (CI)", "screenshots", id="CIscreen24")))
((("screenshots", id="screen24")))
((("debugging", "screenshots for", id="DBscreen24")))
((("HTML", "screenshot dumps", id="HTMLscreen24")))
To be able to debug unexpected failures that happen on a remote server,
it would be good to see a picture of the screen at the moment of the failure,
and maybe also a dump of the HTML of the page.

We can do that using some custom logic in our FT class `tearDown`.
We'll need to do a bit of introspection of `unittest` internals,
a private attribute called `._outcome`,
but this will work:

[role="sourcecode"]
.src/functional_tests/base.py (ch23l006)
====
[source,python]
----
import os
import time
from datetime import datetime
from pathlib import Path
[...]
MAX_WAIT = 5

SCREEN_DUMP_LOCATION = Path(__file__).absolute().parent / "screendumps"
[...]

    def tearDown(self):
        if self._test_has_failed():
            if not SCREEN_DUMP_LOCATION.exists():
                SCREEN_DUMP_LOCATION.mkdir(parents=True)
            self.take_screenshot()
            self.dump_html()
        self.browser.quit()
        super().tearDown()

    def _test_has_failed(self):
        # slightly obscure but couldn't find a better way!
        return self._outcome.result.failures or self._outcome.result.errors
----
====


We first create a directory for our screenshots if necessary.
Then we iterate through all the open browser tabs and pages,
and use a Selenium methods, `get_screenshot_as_file()`
and the attribute `browser.page_source`,
for our image and HTML dumps, respectively:

[role="sourcecode"]
.src/functional_tests/base.py (ch23l007)
====
[source,python]
----
    def take_screenshot(self):
        path = SCREEN_DUMP_LOCATION / self._get_filename("png")
        print("screenshotting to", path)
        self.browser.get_screenshot_as_file(str(path))

    def dump_html(self):
        path = SCREEN_DUMP_LOCATION / self._get_filename("html")
        print("dumping page HTML to", path)
        path.write_text(self.browser.page_source)
----
====


And finally here's a way of generating a unique filename identifier,
which includes the name of the test and its class, as well as a timestamp:

[role="sourcecode small-code"]
.src/functional_tests/base.py (ch23l008)
====
[source,python]
----
    def _get_filename(self, extension):
        timestamp = datetime.now().isoformat().replace(":", ".")[:19]
        return (
            f"{self.__class__.__name__}.{self._testMethodName}-{timestamp}.{extension}"
        )
----
====

You can test this first locally by deliberately breaking one of the tests,
with a `self.fail()` for example, and you'll see something like this:

[role="dofirst-ch21l009"]
----
[...]
.Fscreenshotting to ...goat-book/src/functional_tests/screendumps/MyListsTest.t
est_logged_in_users_lists_are_saved_as_my_lists-[...]
dumping page HTML to ...goat-book/src/functional_tests/screendumps/MyListsTest.
test_logged_in_users_lists_are_saved_as_my_lists-[...]
----

Revert the `self.fail()`, then commit and push:

[role="dofirst-ch21l010"]
[subs="specialcharacters,quotes"]
----
$ *git diff*  # changes in base.py
$ *echo "src/functional_tests/screendumps" >> .gitignore*
$ *git commit -am "add screenshot on failure to FT runner"*
$ *git push*
----

* TODO resume here

And when we rerun the build on Gitlab, we see something like this:

[role="skipme small-code"]
----
screenshotting to ./builds/hjwp/book-example/functional_tests/
screendumps/LoginTest.test_can_get_email_link_to_log_in-window0-2014-01-22T17.45.12.png
dumping page HTML to ./builds/hjwp/book-example/functional_tests/
screendumps/LoginTest.test_can_get_email_link_to_log_in-window0-2014-01-22T17.45.12.html
----

We can go and visit these in the "workspace", which is the folder Jenkins
uses to store our source code and run the tests in, as in
<<screenshots-in-workspace>>.

[[screenshots-in-workspace]]
[role="width-75"]
.Visiting the project workspace
image::images/twp2_2410.png["workspace files including screenshot"]

And then we look at the screenshot, as shown in <<normal-screenshot>>.

[[normal-screenshot]]
[role="width-75"]
.Screenshot looking normal
image::images/twp2_2411.png["Screenshot of site page"]



=== If in Doubt, Try Bumping the Timeout!

((("", startref="CIscreen24")))((("", startref="screen24")))((("", startref="DBscreen24")))((("", startref="HTMLscreen24")))((("Continuous Integration (CI)", "timeout bumping")))((("Jenkins", "timeout bumping")))Hm.  No obvious clues there.  Well, when in doubt, bump the timeout, as the
old adage goes:

[role="sourcecode skipme"]
.src/functional_tests/base.py
====
[source,python]
----
MAX_WAIT = 20
----
====

Then we can rerun the build on Jenkins using "Build Now", and confirm it now
works, as in <<outlook-brighter>>.

[[outlook-brighter]]
[role="width-75"]
.The outlook is brighter
image::images/twp2_2412.png["Build showing a recent pass and sun-peeking-through-clouds logo"]

Jenkins uses blue to indicate passing builds rather than green, which is a bit
disappointing, but look at the sun peeking through the clouds:  that's cheery!
It's an indicator of a moving average ratio of passing builds to failing
builds.  Things are looking up!





Running Our QUnit JavaScript Tests in Jenkins with PhantomJS
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

((("Continuous Integration (CI)", "QUnit JavaScript tests", id="CIqunit24")))((("Jenkins", "QUnit JavaScript tests with", id="Jqunit24")))((("PhantomJS", id="phantom24")))((("QUnit", id="qunit24")))((("JavaScript testing", "in Jenkins with PhantomJS", secondary-sortas="Jenkins", id="JSTjenkins24")))There's
a set of tests we almost forgot--the JavaScript tests. Currently
our "test runner" is an actual web browser.  To get Jenkins to run them, we
need a command-line test runner.  Here's a chance to use PhantomJS.


Installing node
^^^^^^^^^^^^^^^

It's time to stop pretending we're not in the JavaScript game.  We're doing
web development.  That means we do JavaScript.  That means we're going to end
up with node.js on our computers.  It's just the way it has to be.

Follow the instructions on the http://nodejs.org/[node.js homepage]. There are
installers for Windows and Mac, and repositories for popular Linux
distros.footnote:[Make sure you get the latest version. On Ubuntu, use the PPA
rather than the default package.]

Once we have node, we can install phantom:

[role="skipme"]
[subs="specialcharacters,quotes"]
----
$ *npm install -g phantomjs-prebuilt*  # the -g means "system-wide".
----

Next we pull down a QUnit/PhantomJS test runner.  There are several out there
(I even wrote a basic one to be able to test the QUnit listings in this book), 
but the best one to get is probably the one that's linked from the
http://qunitjs.com/plugins/[QUnit plugins page]. At the time of writing, its
repo was at https://github.com/jonkemp/qunit-phantomjs-runner.  The only file
you need is 'runner.js'.

You should end up with this:

[role="skipme"]
[subs="specialcharacters,quotes"]
----
$ *tree src/lists/static/tests/*
src/lists/static/tests/
â”œâ”€â”€ qunit-2.0.1.css
â”œâ”€â”€ qunit-2.0.1.js
â”œâ”€â”€ runner.js
â””â”€â”€ tests.html

0 directories, 4 files
----

Let's try it out:

[role="skipme"]
[subs="specialcharacters,quotes"]
----
$ *phantomjs src/lists/static/tests/runner.js src/lists/static/tests/tests.html*
Took 24ms to run 2 tests. 2 passed, 0 failed.
----

Just to be sure, let's deliberately break something:

[role="sourcecode skipme"]
.src/lists/static/tests/Spec.js (ch23l019)
====
[source,javascript]
----
  it("sense-check our html fixture", () => {
    expect(errorMsg.checkVisibility()).toBe(false);
  });
----
====

Sure enough:

[role="skipme"]
[subs="specialcharacters,quotes"]
----
$ *phantomjs lists/static/tests/runner.js lists/static/tests/tests.html*

Test failed: errors should be hidden on keypress
    Failed assertion: expected: false, but was: true
file://...goat-book/lists/static/tests/tests.html:27:15

Took 27ms to run 2 tests. 1 passed, 1 failed.
----

All right!  Let's unbreak that, commit and push the runner, and then add it to
our [keep-together]#Jenkins# build:

[role="skipme"]
[subs="specialcharacters,quotes"]
----
$ *git checkout lists/static/list.js*
$ *git add lists/static/tests/runner.js*
$ *git commit -m "Add phantomjs test runner for javascript tests"*
$ *git push* 
----


Adding the Build Steps to Jenkins
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Edit the project configuration again, and add a step for each set of 
JavaScript tests, as per <<js-unit-tests-jenkey>>.


[[js-unit-tests-jenkey]]
.Add a build step for our JavaScript unit tests
image::images/new_jenkins_phantomjs_screenshot.png["JS test runner setup"]

You'll also need to install PhantomJS on the server:

[role="skipme"]
[subs="specialcharacters,quotes"]
----
root@server:$ *add-apt-repository -y ppa:chris-lea/node.js*
root@server:$ *apt update*
root@server:$ *apt install nodejs*
root@server:$ *npm install -g phantomjs-prebuilt*
----

And there we are!  A complete CI build featuring all of our tests!


[role="skipme"]
----
Started by user harry
Building in workspace ./builds/hjwp/book-examplekspace
Fetching changes from the remote Git repository
Fetching upstream changes from https://github.com/hjwp/book-example.git
Checking out Revision 936a484038194b289312ff62f10d24e6a054fb29 (origin/chapter_1
Xvfb starting$ /usr/bin/Xvfb :1 -screen 0 1024x768x24 -fbdir /var/lib/jenkins/20
[workspace] $ /bin/sh -xe /tmp/shiningpanda7092102504259037999.sh

+ pip install -r requirements.txt
[...]

+ python manage.py test lists
.................................
 ---------------------------------------------------------------------
Ran 43 tests in 0.229s

OK
Creating test database for alias 'default'...
Destroying test database for alias 'default'...

+ python manage.py test accounts
..................
 ---------------------------------------------------------------------
Ran 18 tests in 0.078s

OK
Creating test database for alias 'default'...
Destroying test database for alias 'default'...

[workspace] $ /bin/sh -xe /tmp/hudson2967478575201471277.sh
+ phantomjs lists/static/tests/runner.js lists/static/tests/tests.html
Took 32ms to run 2 tests. 2 passed, 0 failed.
+ phantomjs lists/static/tests/runner.js accounts/static/tests/tests.html
Took 47ms to run 11 tests. 11 passed, 0 failed.

[workspace] $ /bin/sh -xe /tmp/shiningpanda7526089957247195819.sh
+ pip install selenium
Requirement already satisfied (use --upgrade to upgrade): selenium in /var/lib/

Cleaning up...
[workspace] $ /bin/sh -xe /tmp/shiningpanda2420240268202055029.sh
+ python manage.py test functional_tests
........
 ---------------------------------------------------------------------
Ran 8 tests in 76.804s

OK
----




((("", startref="CIqunit24")))
((("", startref="Jqunit24")))
((("", startref="phantom24")))
((("", startref="qunit24")))
((("", startref="JSTjenkins24")))
Nice to know that, no matter how lazy I get about running the full test suite
on my own machine, the CI server will catch me.  Another one of the Testing
Goat's agents in cyberspace, watching over us...



More Things to Do with a CI Server
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

((("Continuous Integration (CI)", "additional uses for")))I've
only scratched the surface of what you can do with Jenkins and CI servers.
For example, you can make it much smarter about how it monitors your repo for
new commits.footnote:[See this reader's 
https://blog.longearsfor.life/blog/2018/07/30/jenkins-and-github-integration-testinggoat-book/[blog post on the topic for example]]

Perhaps more interestingly, you can use your CI server to automate your staging
tests as well as your normal functional tests.  If all the FTs pass, you can
add a build step that deploys the code to staging, and then reruns the FTs 
against that--automating one more step of the process, and ensuring that your
staging server is automatically kept up to date with the latest code.

Some people even use a CI server as the way of deploying their production
releases!


.Tips on CI and Selenium Best Practices
*******************************************************************************

Set up CI as soon as possible for your project::
    ((("Selenium", "best CI practices")))((("Continuous Integration (CI)", "tips")))As
soon as your functional tests take more than a few seconds to run,
    you'll find yourself avoiding running them all. Give this job to a CI
    server, to make sure that all your tests are getting run somewhere.
    

Set up screenshots and HTML dumps for failures::
    ((("screenshots")))((("debugging", "screenshots for")))((("HTML", "screenshot dumps")))Debugging
test failures is easier if you can see what the page looked
    like when the failure occurred.  This is particularly useful for debugging
    CI failures, but it's also very useful for tests that you run locally.

Be prepared to bump your timeouts::
    A CI server may not be as speedy as your laptop, especially if it's under
    load, running multiple tests at the same time.  Be prepared to be even
    more generous with your timeouts, in order to minimise the chance of
    random failures.

Look into hooking up CI and staging::
    ((("Continuous Integration (CI)", "staging and")))((("staging sites", "continuous integrations and")))Tests
that use `LiveServerTestCase` are all very well for dev boxes,
    but the true reassurance comes from running your tests against a real 
    server.  Look into getting your CI server to deploy to your staging server,
    and run the functional tests against that instead.  It has the side benefit
    of testing your automated deploy scripts.((("", startref="CI24")))
    

*******************************************************************************

